{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Model using Keras\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv('./input/MatrixData', sep='\\t')\n",
    "# Convert label to something useful\n",
    "data.dropna(inplace=True)\n",
    "def change_labels(x): return 1 if x == \"spam\" else 0\n",
    "data['label'] = data['label'].apply(change_labels)\n",
    "\n",
    "sentences = data['message'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# Separate out the sentences and labels into training and test sets\n",
    "training_size = int(len(sentences) * 0.8)\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "\n",
    "# Make labels into numpy arrays for use with the network later\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type,\n",
    "                       truncating=trunc_type)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length,\n",
    "                               padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Dropout(.3,),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dropout(.3,),\n",
    "    tf.keras.layers.Dense(6, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    tf.keras.layers.Dropout(.3,),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "# Define the checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Define the name of the checkpoint files.\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 105\n",
    "history = model.fit(padded, \n",
    "                    training_labels_final, \n",
    "                    epochs=num_epochs, \n",
    "                    verbose=0, \n",
    "                    callbacks=[tensorboard_callback, \n",
    "                                tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                                   save_weights_only=True),\n",
    "                    ],\n",
    "                    validation_data=(testing_padded, testing_labels_final))\n",
    "\n",
    "print(\"Average test loss: \", np.average(history.history['loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import time\n",
    "model.save(f\"./models/spam_keras_{time.time()}\")\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict whether a message is spam\n",
    "text_messages = ['Greg, can you call me back once you get this?',\n",
    "                 'Congrats on your new iPhone! Click here to claim your prize...',\n",
    "                 'Really like that new photo of you',\n",
    "                 'Did you hear the news today? Terrible what has happened...',\n",
    "                 'Attend this free COVID webinar today: Book your session now...',\n",
    "                 'Are you coming to the party tonight?',\n",
    "                 'Your parcel has gone missing',\n",
    "                 'Do not forget to bring friends!',\n",
    "                 'You have won a million dollars! Fill out your bank details here...',\n",
    "                 'Looking forward to seeing you again',\n",
    "                 'oh wow https://github.com/MGCodesandStats/tensorflow-nlp/blob/master/spam%20detection%20tensorflow%20v2.ipynb works really good on spam detection. Guess I go with that as the base model then lol :D',\n",
    "                 'ayo',\n",
    "                 'Almost all my spam is coming to my non-gmail address actually',\n",
    "                 'Oh neat I think I found the sizing sweetspot for my data :D',\n",
    "                 'would never click on buttons in gmail :D always expecting there to be a bug in gmail that allows js to grab your google credentials :D XSS via email lol. I am too scared for touching spam in gmail']\n",
    "\n",
    "print(text_messages)\n",
    "\n",
    "# Create the sequences\n",
    "padding_type = 'post'\n",
    "sample_sequences = tokenizer.texts_to_sequences(text_messages)\n",
    "fakes_padded = pad_sequences(\n",
    "    sample_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "classes = model.predict(fakes_padded)\n",
    "\n",
    "# The closer the class is to 1, the more likely that the message is spam\n",
    "for x in range(len(text_messages)):\n",
    "  print(text_messages[x])\n",
    "  print(classes[x])\n",
    "  print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
